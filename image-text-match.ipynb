{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.9.0+cu111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenan/.conda/envs/clip-torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f5d9266b040>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_triplet</th>\n",
       "      <th>neg_triplet</th>\n",
       "      <th>pos_url</th>\n",
       "      <th>neg_url</th>\n",
       "      <th>pos_image_id</th>\n",
       "      <th>neg_image_id</th>\n",
       "      <th>subj_neg</th>\n",
       "      <th>verb_neg</th>\n",
       "      <th>obj_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Girl is standing in the grass.</td>\n",
       "      <td>girl,stand,grass</td>\n",
       "      <td>dog,stand,grass</td>\n",
       "      <td>https://previews.123rf.com/images/aleksejzhagu...</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman lies on a beach.</td>\n",
       "      <td>woman,lie,beach</td>\n",
       "      <td>woman,jump,beach</td>\n",
       "      <td>https://media.istockphoto.com/photos/man-and-w...</td>\n",
       "      <td>https://media.istockphoto.com/photos/woman-to-...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence       pos_triplet       neg_triplet  \\\n",
       "0  Girl is standing in the grass.  girl,stand,grass   dog,stand,grass   \n",
       "1        A woman lies on a beach.   woman,lie,beach  woman,jump,beach   \n",
       "\n",
       "                                             pos_url  \\\n",
       "0  https://previews.123rf.com/images/aleksejzhagu...   \n",
       "1  https://media.istockphoto.com/photos/man-and-w...   \n",
       "\n",
       "                                             neg_url  pos_image_id  \\\n",
       "0  https://images-na.ssl-images-amazon.com/images...             0   \n",
       "1  https://media.istockphoto.com/photos/woman-to-...             2   \n",
       "\n",
       "   neg_image_id  subj_neg  verb_neg  obj_neg  \n",
       "0             1      True     False    False  \n",
       "1             3     False      True    False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "svo_cleaned = pd.read_csv('svo_valid.csv')\n",
    "svo_cleaned.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from PIL import UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenan/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/kenan/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 2. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "image file is truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/ImageFile.py:239\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     s \u001b[39m=\u001b[39m read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecodermaxblock)\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, struct\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    241\u001b[0m     \u001b[39m# truncated png/gif\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/PngImagePlugin.py:932\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(\u001b[39m4\u001b[39m)  \u001b[39m# CRC\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m cid, pos, length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpng\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m cid \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIDAT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDDAT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfdAT\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/PngImagePlugin.py:177\u001b[0m, in \u001b[0;36mChunkStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mtell()\n\u001b[0;32m--> 177\u001b[0m     length \u001b[39m=\u001b[39m i32(s)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cid(cid):\n",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/_binary.py:85\u001b[0m, in \u001b[0;36mi32be\u001b[0;34m(c, o)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mi32be\u001b[39m(c, o\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m unpack_from(\u001b[39m\"\u001b[39;49m\u001b[39m>I\u001b[39;49m\u001b[39m\"\u001b[39;49m, c, o)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/kenan/CLIP/svo-probes/image-text-match.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkobe-4u-3.soe.ucsc.edu/home/kenan/CLIP/svo-probes/image-text-match.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39msentence\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkobe-4u-3.soe.ucsc.edu/home/kenan/CLIP/svo-probes/image-text-match.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m image_pos \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(skimage\u001b[39m.\u001b[39mdata_dir, image_path\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(row\u001b[39m.\u001b[39mpos_image_id) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkobe-4u-3.soe.ucsc.edu/home/kenan/CLIP/svo-probes/image-text-match.ipynb#ch0000004vscode-remote?line=6'>7</a>\u001b[0m image_neg \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39;49mopen(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(skimage\u001b[39m.\u001b[39;49mdata_dir, image_path\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(row\u001b[39m.\u001b[39;49mneg_image_id) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkobe-4u-3.soe.ucsc.edu/home/kenan/CLIP/svo-probes/image-text-match.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m images \u001b[39m=\u001b[39m [image_pos, image_neg]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkobe-4u-3.soe.ucsc.edu/home/kenan/CLIP/svo-probes/image-text-match.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m image_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack(images))\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/Image.py:901\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m    858\u001b[0m ):\n\u001b[1;32m    859\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    903\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    905\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clip-torch/lib/python3.9/site-packages/PIL/ImageFile.py:245\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mimage file is truncated\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s:  \u001b[39m# truncated jpeg\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[0;31mOSError\u001b[0m: image file is truncated"
     ]
    }
   ],
   "source": [
    "matched_id = [] ## if cosine similarity to positive image larger: 0, if not: 1\n",
    "image_path = \"/data1/kenan/SVO_Probes/images/\"\n",
    "for idx, row in svo_cleaned.iterrows():\n",
    "    try:\n",
    "        text = row.sentence\n",
    "        image_pos = preprocess(Image.open(os.path.join(skimage.data_dir, image_path+str(row.pos_image_id) + \".jpg\")).convert(\"RGB\"))\n",
    "        image_neg = preprocess(Image.open(os.path.join(skimage.data_dir, image_path+str(row.neg_image_id) + \".jpg\")).convert(\"RGB\"))\n",
    "        images = [image_pos, image_neg]\n",
    "        image_input = torch.tensor(np.stack(images)).cuda()\n",
    "        text_tokens = clip.tokenize([text]).cuda()\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).float()\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "        matched_id.append(np.argmax(similarity))\n",
    "    except UnidentifiedImageError:\n",
    "        matched_id.append(\"image_failed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35763\n",
      "35763\n"
     ]
    }
   ],
   "source": [
    "print(len(matched_id))\n",
    "print(len(svo_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_cleaned = svo_cleaned[:35763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(matched_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_cleaned[\"matched_id\"] = matched_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_cleaned = svo_cleaned[svo_cleaned.matched_id != \"image_failed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6047"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(svo_cleaned[\"matched_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32938"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svo_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_cleaned.to_csv(\"clip_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = svo_cleaned[svo_cleaned[\"verb_neg\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20810"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4382"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(verb[\"matched_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = svo_cleaned[svo_cleaned[\"subj_neg\"] == True]\n",
    "print(len(subject))\n",
    "sum(list(subject[\"matched_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "815"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object = svo_cleaned[svo_cleaned[\"obj_neg\"] == True]\n",
    "print(len(object))\n",
    "sum(list(object[\"matched_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('clip-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88beffe3055ef9797590ee073211e9aee9c8621e04d2e6cfacd27b0cbc9abf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
